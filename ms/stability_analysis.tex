\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{lineno}
\linenumbers
% math typesetting 
\let\vec\mathbf
\newcommand{\matrx}[1]{{\left[ \stackrel{}{#1}\right]}}

%SetFonts

%SetFonts

\pagestyle{headings}
\markright{Yoav Ram, \today \; v1\hfill}

\newcommand{\beginsupplement}{%
	\setcounter{table}{0}
	\renewcommand{\thetable}{A\arabic{table}}%
	\setcounter{figure}{0}
	\renewcommand{\thefigure}{A\arabic{figure}}%
	\setcounter{equation}{0}
	\renewcommand{\theequation}{A\arabic{equation}}%
}


\begin{document}
\beginsupplement

Here I describe how I analyzed the stability of a modifier allele $m$ with vertical transmission rate $\rho$ to invasion by a modifier $M$ with a vertical transmission rate $P$, as described in the main text, eqs. 34-35, in environmental regime $AkBl$.
\\
The analysis is similar to that used in the main text to analyze stability in $A1B1$, but it uses computation because the cases where $k>1$ or $l>1$ cannot be analyzed using closed form expressions.
\\
The analysis includes the following steps for a fixed $W, w, k,$ and $l$:
\begin{enumerate}

\item Find the stable frequency vector $\vec{x^*}$ without invader $M$, i.e. $x^*_3=x^*_4=0$. This is done by minimizing the expression $|x_{k+l} - x_{0}|$ where $x_{t}$ is defined in eq. 9 of the main text. The minimization is done by iterating the recurrence until it converges, i.e. until the difference is smaller than machine precision (roughly $10^-8$ when subtracting similar small numbers).

\item Define $F_A(x)$ by eq. 32 of main text with $w_A=W$ and $w_B=w$ ($W>w$), and similarly $F_B(x)$ with $w_B=W$ and $w_A=w$.

\item Define, similar to eq. 34 of main text,
\begin{equation}
\vec{F} = \underbrace{F_B \circ \ldots \circ F_B}_{l \text{ times}} \circ
	\underbrace{F_A \circ \ldots \circ F_A}_{k \text{ times}}.
\label{eq:F}\end{equation}

\item Calculate the linear approximation $\vec{J}$ of $\vec{F}$  near $\vec{x^*}$, that is, the Jacobian matrix of $\vec{F}$ at $\vec{x^*}$:
\begin{equation}
\vec{J}_{ij} = \frac{\partial F_i}{\partial x_j}.
\label{eq:jacobian}\end{equation}

\item Define $\vec{L}=\vec{L_{ex}}$ as in eqs. 40-45 in the main text such that
\begin{equation}
\vec{J} = \begin{pmatrix}
\vec{L_{in}} & * \\
* & \vec{L_{ex}}
\end{pmatrix}.
\end{equation}

\item Calculate the eigenvalues $\lambda_1 > \lambda_2$ of $\vec{L}$; by the Perron-Frobenius theorem, the leading eigenvalue $\lambda_1$ is real and positive. Denote $\lambda_1(\rho, P)$ the resulting leading eigenvalue with resident rate $\rho$ and invader rate $P$. Note that 
\begin{equation}
\lambda_1(\rho, \rho) = 1,
\end{equation}
for any $\rho \in (0,1)$.

\item The evolutionary stable rate $\rho^*$ is defined as stable to invasion, that is, 
\begin{equation}
\lambda_1(\rho^*,\rho^* \pm dP) < 1 = \lambda_1(\rho^*,\rho^*)
\end{equation}
for a small enough $dP>0$.
Therefore, 
\begin{equation}
\frac{\partial \lambda_1}{\partial P}\big(\rho^*,\rho^*\big) = 0. 
\label{eq:dλdP=0} \end{equation}

\item We use Brent's root-finding method (Brent, 1971) to find $\rho^*$ that satisfies \autoref{eq:dλdP=0}. If, due to numerical instability of the process, we have
\begin{equation}
\frac{\partial \lambda_1}{\partial P}\big(0,0\big) \cdot \frac{\partial \lambda_1}{\partial P}\big(1,1) > 0,
\end{equation}
i.e., the partial derivative sign is identical at $\rho=P=0$ and $\rho=P=1$, then we cannot use Brent's method. In these cases we assume that the partial derivative doesn't have a root in $(0,1)$ and we determine the stable rate $\rho^*$ by the rule
\begin{equation}
\rho^* = \begin{cases}
0, \quad \text{if } \frac{\partial \lambda_1}{\partial P}\big(0,0\big) \le 0, \\
1, \quad \text{if } \frac{\partial \lambda_1}{\partial P}\big(0,0\big) > 0.
\end{cases}
\end{equation}

\end{enumerate}

\paragraph{Results}
\autoref{fig:AkBk_stable_modifier_w_0.1}, \autoref{fig:AkBk_stable_modifier_w_0.5} and \autoref{fig:AkBk_stable_modifier_w_0.9}, show the leading eigenvalue $\lambda_1$ of the external stability matrix $\vec{L}$ for different choices of environmental cycles $AkBk$, resident rate $\rho$ and invader rate $P$. The arrows show a series of invading modifiers generated using Algorithm~\autoref{algorithm} (this algorithm cannot be efficiently used to determine the stable rate as it requires fine tuning of the meta-parameter $\eta$).

The leading eigenvalue calculation is good for small $k$ but for large $k$ and especially for $w=0.1$ the calculation is unstable.
This is probably because when the environment is constant for a long period of time, the frequencies are close to the boundaries (i.e. 0 and 1) and the gradient just after the environment changes is very sharp.
Therefore, the calculation of $\vec{J}$ is unstable.
Also, because most of the time the frequencies don't change by much, the elements of $\vec{L}$ are small, leading to unstable calculation of the leading eigenvalue.
\\

\begin{algorithm} 
% https://en.wikibooks.org/wiki/LaTeX/Algorithms
\caption{Modifier gradient ascent algorithm}\label{algorithm}
\begin{algorithmic}[1]
\State $\eta > 0$ is a step size
\State $\rho_{0} = 0.5$ is a starting condition
\State $t = 0$ is an iteration counter
\While {$\frac{|\rho_{t} - \rho_{t-1}|}{\rho_{t-1}} < \epsilon \;\text{and}\; \eta > \eta_0$}
	\State $P_{t} \gets \rho_{t} + \eta \frac{\partial \lambda_1(\rho_{t}, \rho_{t})}{\partial P}$
	\If {$\lambda_1(\rho_{t}, P_{t})$} 
	\State {$\rho_{t+1} \gets P_{t}$}
	\Else
	\State {$\eta \gets \eta / 2$}
	\EndIf
	\State $t \gets t+1$
\EndWhile
\State $\rho^* \gets \rho_t$
\end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{../figures/AkBk_stable_optimal_rate.pdf}
  \caption{Fitness "optimal" and evolutionary stable vertical transmission rate in $AkBk$ selection regime. \textbf{(A)} The vertical transmission rate $\hat{rho}$ that maximized the geometric average of the population mean fitness is zero, i.e. complete oblique transmission, when selection cycles quickly between favoring phenotype $A$ and $B$, and then abruptly transitions to ~0.2, followed by a slow decrease (see Fig. SX for details on the abrupt transition). 
  \textbf{(B)} The evolutionary stable rate $\rho^*$ which cannot be invaded by modifiers with either higher or lower vertical transmission rate $P$, rapidly increases from zero when selection cycles are short ($k=1$ or $2$) to roughly 1 when the environment cycles are longer. The stable rate was found by finding the root of $\frac{\partial \lambda_1}{\partial P}$, the leading eigenvalue of the external stability matrix w.r.t to the rate of the invading modifier (see Supplementary Information for details). The dashed lines shows $1-\frac{1}{k-1}$, which seems to fit the values for $w=0.5$ (Carja et al. 2011). The values for $w=0.1$ (blue) could not be calculated for $k>21$ due to numerical instability when selection is strong and the duration between selection fluctuations is long.
  In all cases, $W=1$}
  \label{fig:AkBk_stable_modifier_w_0.1}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{{../figures/AkBk_stable_modifier_w_0.1}.pdf}
  \caption{The leading eigenvalue of the external stability matrix in $AkBk$ with selection. The colors show $\lambda_1(W, w, \rho, P, k, l)$, the leading eigenvalue of the external stability matrix for combinations of $\rho$ (y-axis) and $P$ (x-axis), the vertical transmission rates of resident and invader modifiers, respectively. Purple shows values lower than 1, leading to stability of $\rho$; white equals to 1; green larger than 1, leading to invasion of $P$.
  The arrows show an "evolutionary" path $\rho_{0} \to \rho_{1} \to \ldots \to \rho^*$: at each step, a modifier with rate $P_t$ invades the resident modifier with rate $\rho_t$ and becomes the resident modifier $\rho_{t+1}$, until consecutive invaders have similar rates (see Algorithm~\autoref{algorithm}).
  Here, $W=1$ and $w=0.1$.}
  \label{fig:AkBk_stable_modifier_w_0.1}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{{../figures/AkBk_stable_modifier_w_0.5}.pdf}
  \caption{The leading eigenvalue of the external stability matrix in $AkBk$ with intermediate selection.
  Same as~\autoref{fig:AkBk_stable_modifier_w_0.1} except here, $W=1$ and $w=0.5$.}
  \label{fig:AkBk_stable_modifier_w_0.5}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\linewidth]{{../figures/AkBk_stable_modifier_w_0.9}.pdf}
  \caption{The leading eigenvalue of the external stability matrix in $AkBk$ with intermediate selection.
  Same as~\autoref{fig:AkBk_stable_modifier_w_0.1} except here, $W=1$ and $w=0.9$.}
  \label{fig:AkBk_stable_modifier_w_0.9}
\end{figure}

\paragraph{Note}
Crucially, the Jacobian $\vec{J}$ in~\autoref{eq:jacobian} can be calculated using \textit{automatic differentiation} with good precision from a function that iteratively calculates $\vec{F}$ according to~\autoref{eq:F}. Similarly, the partial derivative $\frac{\partial \lambda_1}{\partial P}$ in Algorithm~\autoref{algorithm} can be calculated from a function that calculates $\lambda_1$ using simple arithmetic operations, as the eigenvalues can be found from a quadratic formula.
\\
Note that \textit{automatic differentiation} does not mean \textit{symbolic} or \textit{numerical differentiation}, which can lead to inefficient or inaccurate estimation of $\vec{J}$ when $k$ is not very small. Rather, from Bartholomew-Biggs et al. (2000): 
\begin{quotation}\textit{
Automatic differentiation is a set of techniques for transforming a program that calculates numerical values of a function, into a program which calculates numerical values for derivatives of that function with about the same accuracy and efficiency as the function values themselves.
}\end{quotation}

\paragraph{References} ~\\
\begin{itemize}

\item Bartholomew-Biggs, M., S. Brown, B. Christianson, and L. Dixon. 2000. Automatic differentiation of algorithms. J. Comput. Appl. Math. 124:171–190.

\item Brent, R. P. 1971. An algorithm with guaranteed convergence for finding a zero of a function. Comput. J. 14:422–425.

\end{itemize}

\end{document}  